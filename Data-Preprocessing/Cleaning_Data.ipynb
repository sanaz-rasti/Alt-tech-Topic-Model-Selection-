{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/0pnst_4s44b07dt0xbhvxlv80000gn/T/ipykernel_3834/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/0pnst_4s44b07dt0xbhvxlv80000gn/T/ipykernel_3834/321058008.py:33: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import sqlite3 as lite\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import logging\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Database connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''the path to the database'''\n",
    "db_path1='TelegramData.db'\n",
    "\n",
    "logging.info('Saving recipes to {}'.format(db_path1))\n",
    "db = create_engine('sqlite:///{}'.format(db_path1)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Relevant Channels from Classified_ChannelDetail table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1001250192398, -1001486710610, -1001719423078, -1001522159637, -1001242115487, -1001198445854]\n"
     ]
    }
   ],
   "source": [
    "validated_Channels=pd.read_sql(\"Classified_ChannelDetail\",con=db)\n",
    "validated_Channels=validated_Channels.loc[validated_Channels[\"Relevant?\"]==\"Yes\"]\n",
    "List_of_Valid_Channel=validated_Channels.Channel_ID.tolist()\n",
    "#print(List_of_Valid_Channel)\n",
    "list_6irish_channels = [-1001250192398,-1001486710610,-1001719423078, -1001522159637,-1001242115487,-1001198445854]\n",
    "print(list_6irish_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Text Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content_1 = pd.read_sql(\"TextContent_Telegram_Channel\",con=db)\n",
    "# reading from the first table\n",
    "text_content_1 = text_content_1.loc[text_content_1[\"Channel_ID\"].isin(list_6irish_channels)]\n",
    "#print(len(text_content_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(text_content_1.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205707\n"
     ]
    }
   ],
   "source": [
    "# reading from the second table\n",
    "text_content_2 = pd.read_sql(\"TextContent_Telegram_Channel_2\",con=db)\n",
    "text_content_2 = text_content_2.loc[text_content_2[\"Channel_ID\"].isin(list_6irish_channels)]\n",
    "print(len(text_content_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/0pnst_4s44b07dt0xbhvxlv80000gn/T/ipykernel_20762/1843917651.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  TextCorpus = text_content_1.append(text_content_2)\n"
     ]
    }
   ],
   "source": [
    "# merging the both table'''\n",
    "TextCorpus = text_content_1.append(text_content_2)\n",
    "print(len(TextCorpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Duplicate Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextCorpus_unique = TextCorpus.drop_duplicates(['Channel_ID', 'MessageContent', 'MessageID', 'MessageDate', 'Message_FROm_UserID'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_Corpus_Refine = TextCorpus_unique\n",
    "Text_Corpus_Refine = Text_Corpus_Refine.loc[ ~(Text_Corpus_Refine['MessageContent'].str.match(r'https?[:;]?/?/?\\S*')) ]\n",
    "Text_Corpus_Refine = Text_Corpus_Refine.loc[ ~(Text_Corpus_Refine['MessageContent'].str.contains(\"Tnx for joining\")) ]\n",
    "Text_Corpus_Refine = Text_Corpus_Refine.loc[ ~(Text_Corpus_Refine['MessageContent'].str.contains(\"Thanks for joining\")) ]\n",
    "Text_Corpus_Refine = Text_Corpus_Refine.loc[ ~(Text_Corpus_Refine['MessageContent'].str.contains(\"Thank you to all the new members for joining\")) ]\n",
    "Text_Corpus_Refine = Text_Corpus_Refine.loc[ ~(Text_Corpus_Refine['MessageContent'].str.contains(\"Thank u x\")) ]\n",
    "Text_Corpus_Refine = Text_Corpus_Refine.loc[ ~(Text_Corpus_Refine['MessageContent'].str.contains(\"Thanks for joining our small group\")) ]\n",
    "Text_Corpus_Refine = Text_Corpus_Refine.loc[ ~(Text_Corpus_Refine['MessageContent'].str.contains(\"Thanks for Joining\")) ]\n",
    "Text_Corpus_Refine = Text_Corpus_Refine.loc[ ~(Text_Corpus_Refine['MessageContent'].str.contains(\" has been banned! Reason: CAS ban\")) ]\n",
    "Text_Corpus_Refine = Text_Corpus_Refine.loc[ ~(Text_Corpus_Refine['MessageContent'].str.contains(\"SEND OUR QR CODE\")) ]\n",
    "Text_Corpus_Refine = Text_Corpus_Refine.loc[ ~(Text_Corpus_Refine['MessageContent'].str.contains(\"SHARE OUR QR CODE\")) ]\n",
    "Text_Corpus_Refine = Text_Corpus_Refine.loc[ ~(Text_Corpus_Refine['MessageContent'].str.contains(\"send our QR code\")) ]\n",
    "Text_Corpus_Refine = Text_Corpus_Refine.loc[ ~(Text_Corpus_Refine['MessageContent'].str.match(\"Check this out..\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186083"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text_Corpus_Refine.to_sql(\"TelegramChannel_CovidOpinion_AcceptedTexts\",con=db,index=False, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186083"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Text_Corpus_Refine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    \n",
    "    tag_dict = {\"J\": wn.ADJ,\n",
    "                \"N\": wn.NOUN,\n",
    "                \"V\": wn.VERB,\n",
    "                \"R\": wn.ADV,\n",
    "                \"S\": wn.ADJ_SAT}\n",
    "    \n",
    "    if tag in tag_dict.keys():\n",
    "        return tag_dict.get(tag, tag_dict[tag])\n",
    "  \n",
    "    else:\n",
    "        return tag_dict.get(tag, wn.NOUN)\n",
    "\n",
    "\n",
    "        \n",
    "def StemLemmatizerText(text_Content):\n",
    "    h=list()\n",
    "    h[:]=[]\n",
    "   \n",
    "    text_Content= text_Content.lower()\n",
    "    \n",
    "    text_Content = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', text_Content)\n",
    "    \n",
    "    text_Content = re.sub('https?[:;]?/?/?\\S*', ' ', text_Content)\n",
    "    \n",
    "    text_Content = re.sub(r'#|!|@|,|‚Ä¢|‚Äú|‚Äù|\\\"|\\'|‚Äôs|n‚Äôt|‚Äô','',text_Content)\n",
    "    \n",
    "    text_Content = re.sub(r'\\?',' ',text_Content)\n",
    "    \n",
    "    text_Content = re.sub('[.] *',' ',text_Content)\n",
    "    \n",
    "    text_Content = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|[0-9]?[0-9]?(\\.[0-9][0-9]?)?|(\\w+:\\/\\/\\S+)|^rt\", \"\", text_Content)\n",
    "    \n",
    "    text_Content = re.sub(r'com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+[ ]?',' ',text_Content)\n",
    "\n",
    "\n",
    "    \n",
    "    for token in [wnl.lemmatize(w, pos=get_wordnet_pos(w)) for w in nltk.word_tokenize(text_Content)]:\n",
    "        if wn.morphy(token):\n",
    "            token=wn.morphy(token)\n",
    "            \n",
    "#         if get_wordnet_pos(token)==wn.NOUN:\n",
    "#             token=p_stemmer.stem(token)\n",
    "\n",
    "        if not token.isdigit() and token not in text.ENGLISH_STOP_WORDS:\n",
    "            h.append(wnl.lemmatize(token,pos=get_wordnet_pos(token)))\n",
    "       \n",
    "    stemed_text_Content=''            \n",
    "    stemed_text_Content=' '.join(h)\n",
    "    return stemed_text_Content\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(StemLemmatizerText(\"AUSTRIA üá¶üáπ: The Austrian government is in trouble? The obligation of imposing the control as directed by the minister of the interior is rejected by the police union https://infodujour.fr/politique/54068-lautriche-se-souleve-contre-la-dictature-sanitaire The union of Armed Forces have annnouced that they will participate in the grand protest in Vienna next Saturday https://twitter.com/Yoann_IV/status/1460687657999155205 The police and the army refuse to make controls for the sanitary pass under the name of ‚Äòliberty and human dignity‚Äô. They will join a large protest against the mandatory lockdown of the non vaccinated on the 20th November in Vienna\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(StemLemmatizerText(\"Are you ready alpha men we will take everything and we will give them nothing üí™ https://t.me/alphamenassemblechannel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df2Process = pd.read_sql(\"TelegramChannel_CovidOpinion_AcceptedTexts\",con=db)\n",
    "print(len(Df2Process))\n",
    "step=10000\n",
    "begin=0\n",
    "end=begin+step\n",
    "\n",
    "while end<len(Df2Process):\n",
    "    \n",
    "    temp_df=Df2Process.loc[begin:end]\n",
    "    temp_df[\"strip_text\"]=''\n",
    "    temp_df[\"semantic_unit_count\"]=''\n",
    "\n",
    "    for i in range(begin, end+1):\n",
    "        if temp_df.at[i,\"MessageContent\"] and StemLemmatizerText(temp_df.at[i,\"MessageContent\"]):\n",
    "            temp_df.at[i,\"strip_text\"] = StemLemmatizerText(temp_df.at[i,\"MessageContent\"])\n",
    "            temp_df.at[i,\"semantic_unit_count\"]=len(nltk.word_tokenize(temp_df.at[i,\"strip_text\"]))\n",
    "    \n",
    "    temp_df.to_sql(\"Telegram_Channel_strippedText_Processed\",con=db,index=False, if_exists=\"replace\")\n",
    "    \n",
    "    begin=end+1\n",
    "    end=end+step    \n",
    "    del(temp_df)\n",
    "    \n",
    "\n",
    "    print(begin,end)\n",
    "\n",
    "#quickFix for the leftover len(Df2Process)%tep\n",
    "LO = pd.read_sql(\"Telegram_Channel_strippedText_Processed\",con=db) \n",
    "\n",
    "temp_df = Df2Process.loc[len(LO)+1:len(Df2Process)-1]\n",
    "temp_df[\"strip_text\"]=''\n",
    "temp_df[\"semantic_unit_count\"]=''\n",
    "\n",
    "for i in range(len(LO)+1, len(Df2Process)):\n",
    "    if temp_df.at[i,\"MessageContent\"] and StemLemmatizerText(temp_df.at[i,\"MessageContent\"]):\n",
    "        temp_df.at[i,\"strip_text\"]= StemLemmatizerText(temp_df.at[i,\"MessageContent\"])\n",
    "        temp_df.at[i,\"semantic_unit_count\"]=len(nltk.word_tokenize(temp_df.at[i,\"strip_text\"]))\n",
    "        \n",
    "\n",
    "temp_df.to_sql(\"Telegram_Channel_strippedText_Processed\",con = db, index=False, if_exists = \"replace\")\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Empty Enteries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176082\n",
      "166653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "166653"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.read_sql(\"Telegram_Channel_strippedText_Processed\",con=db)\n",
    "print(len(temp_df))\n",
    "temp_df = temp_df[temp_df['strip_text']!=\"\"]\n",
    "print(len(temp_df))\n",
    "\n",
    "temp_df.to_sql(\"Telegram_Channel_strippedText_Processed\", con=db, index=False, if_exists=\"replace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
